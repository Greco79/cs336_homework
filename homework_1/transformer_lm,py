import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from einops import rearrange, einsum
from collections.abc import Iterable
from jaxtyping import Float, Int
from torch import Tensor

class Linear(nn.Module):
    """
    Given the weights of a Linear layer, compute the transformation of a batched input.

    Args:
        in_dim (int): The size of the input dimension
        out_dim (int): The size of the output dimension
        weights (Float[Tensor, "d_out d_in"]): The linear weights to use
        in_features (Float[Tensor, "... d_in"]): The output tensor to apply the function to
    
    Returns:
        Float[Tensor, "... d_out"]: The transformed output of your linear module.
    """
    def __init__(self, in_features, out_features, device=None, dtype=None):
        super().__init__()
        factory_kwargs={"device":device,"dtype":dtype}
        weight = torch.empty(out_features, in_features, **factory_kwargs)
        self.W = torch.nn.Parameter(weight)

        std=math.sqrt(2/(in_features+out_features))
        torch.nn.init.trunc_normal_(self.W,mean=0,std=std,a=-3*std,b=3*std)

    def forward(self, x: torch.Tensor) -> torch.Tensor: # Apply the linear transformation to the input.
        return einsum(x,self.W, "... d, o d -> ... o")

class Embedding(nn.Module):
    """
    Given the weights of an Embedding layer, get the embeddings for a batch of token ids.

    Args:
        vocab_size (int): The number of embeddings in the vocabulary
        d_model (int): The size of the embedding dimension
        weights (Float[Tensor, "vocab_size d_model"]): The embedding vectors to fetch from
        token_ids (Int[Tensor, "..."]): The set of token ids to fetch from the Embedding layer
    
    Returns:
        Float[Tensor, "... d_model"]: Batch of embeddings returned by your Embedding layer.
    """
    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):
        super().__init__()
        factory_kwargs={"device":device,"dtype":dtype}
        weight = torch.empty(num_embeddings,embedding_dim,  **factory_kwargs)
        self.W = torch.nn.Parameter(weight)
        torch.nn.init.trunc_normal_(self.W,mean=0,std=1,a=-3,b=3)

    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        return self.W[token_ids]
    

class RMSNorm(nn.Module):
    """Given the weights of a RMSNorm affine transform,
    return the output of running RMSNorm on the input features.

    Args:
        d_model (int): The dimensionality of the RMSNorm input.
        eps: (float): A value added to the denominator for numerical stability.
        weights (Float[Tensor, "d_model"]): RMSNorm weights.
        in_features (Float[Tensor, "... d_model"]): Input features to run RMSNorm on. Can have arbitrary leading
            dimensions.

    Returns:
        Float[Tensor,"... d_model"]: Tensor of with the same shape as `in_features` with the output of running
        RMSNorm of the `in_features`.
    """
    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):
        super().__init__()
        factory_kwargs={"device":device,"dtype":dtype}
        self.eps=eps
        self.scale=nn.Parameter(torch.ones(d_model, **factory_kwargs))


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        in_dtype = x.dtype
        x = x.to(torch.float32)
        # Your code here performing RMSNorm
        rms=x.pow(2).mean(dim=-1,keepdim=True).add(self.eps).sqrt()
        x_norm=x/rms
        result = x_norm*self.scale
        # Return the result in the original dtype
        return result.to(in_dtype)

class SWIGLU(nn.Module):
    """Given the weights of a SwiGLU network, return
    the output of your implementation with these weights.

    Args:
        d_model (int): Dimensionality of the feedforward input and output.
        d_ff (int): Dimensionality of the up-project happening internally to your swiglu.
        w1_weight (Float[Tensor, "d_ff d_model"]): Stored weights for W1
        w2_weight (Float[Tensor, "d_model d_ff"]): Stored weights for W2
        w3_weight (Float[Tensor, "d_ff d_model"]): Stored weights for W3
        in_features (Float[Tensor, "... d_model"]): Input embeddings to the feed-forward layer.

    Returns:
        Float[Tensor, "... d_model"]: Output embeddings of the same shape as the input embeddings.
    """
    def __init__(self, d_model: int, d_ff: int,device=None, dtype=None):
        super().__init__()
        self.d_model=d_model
        self.d_ff=d_ff

        factory_kwargs={"device":device,"dtype":dtype}
        w1=torch.empty(d_ff, d_model,  **factory_kwargs)
        w2=torch.empty(d_model, d_ff,  **factory_kwargs)
        w3=torch.empty(d_ff, d_model,  **factory_kwargs)
        self.w1=nn.Parameter(w1)
        self.w2=nn.Parameter(w2)
        self.w3=nn.Parameter(w3)

    
    def forward(self,x:torch.Tensor) -> torch.Tensor:
        tmp=einsum(x,self.w1, "... d, f d -> ... f")
        silu=tmp*torch.sigmoid(tmp)
        content=einsum(x,self.w3,"... d, f d -> ... f")
        gated=silu*content
        res=einsum(self.w2,gated,"d f, ... f -> ... d")
        return res

class ROPE(nn.Module):
    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):
        super().__init__()
        self.theta=theta
        self.d_k=d_k
        self.max_seq_len=max_seq_len

        self.rotary_dim=d_k-(d_k%2)
        half_dim=self.rotary_dim//2
        inv_freq = 1.0 / (self.theta ** (torch.arange(half_dim, dtype=torch.float32) / half_dim))
        pos=torch.arange(max_seq_len,dtype=torch.float32)
        freqs=torch.outer(pos,inv_freq)

        cos_cached=freqs.cos()
        sin_cached=freqs.sin()

        self.register_buffer("cos_cached",cos_cached,persistent=False)
        self.register_buffer("sin_cached",sin_cached,persistent=False)

    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:
        """
            x: Tensor of shape (..., seq_len, d_k)
            token_positions: Tensor of shape (..., seq_len)
            Returns: Tensor of same shape as x
        """
        x1=x[...,:self.rotary_dim]
        x2=x[...,self.rotary_dim:]

        cos = self.cos_cached[token_positions]
        sin = self.sin_cached[token_positions]

        x1_even=x1[...,::2]
        x1_odd=x1[...,1::2]

        x1_rotated_even=x1_even*cos-x1_odd*sin
        x1_rotated_odd=x1_even*sin+x1_odd*cos

        x1_rotated=torch.stack([x1_rotated_even,x1_rotated_odd],dim=-1)
        x1_rotated=x1_rotated.flatten(-2)

        return torch.cat([x1_rotated,x2], dim=-1)

def soft_max(in_features, dim):
    shifted=in_features-in_features.max(dim=dim,keepdim=True).values
    exps=torch.exp(shifted)
    
    return exps/exps.sum(dim,keepdim=True)

def scaled_dot_product(Q,K,V, mask):
    """
        Q: Float[Tensor, " ... queries d_k"],
        K: Float[Tensor, " ... keys d_k"],
        V: Float[Tensor, " ... values d_v"],
        mask: Float[Tensor, " ... queries keys"] | None = None,
        -> Float[Tensor, " ... queries d_v"]
    """
    scores = Q @ K.transpose(-2, -1)
    d_k=Q.shape[-1]
    scores=scores/math.sqrt(d_k)
    if mask is not None:
        scores=scores.masked_fill(mask==False,float("-inf"))
    weights=torch.softmax(scores,dim=-1) # (queries, keys)
    return weights@V


class CausalMultiheadsSelfAttention(nn.Module):
    """
    Args:
        d_model (int): Dimensionality of the feedforward input and output.
        num_heads (int): Number of heads to use in multi-headed attention.
        max_seq_len (int): Maximum sequence length to pre-cache if your implementation does that.
        q_proj_weight (Float[Tensor, "d_k d_in"]): Weights for the Q projection
        k_proj_weight (Float[Tensor, "d_k d_in"]): Weights for the K projection
        v_proj_weight (Float[Tensor, "d_k d_in"]): Weights for the V projection
        o_proj_weight (Float[Tensor, "d_model d_v"]): Weights for the output projection
        in_features (Float[Tensor, "... sequence_length d_in"]): Tensor to run your implementation on.
    """
    def __init__(self, d_model: int, num_heads: int,device=None, dtype=None):
        super().__init__()
        self.d_model=d_model
        self.num_heads=num_heads
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.dk = self.dv = d_model//num_heads

    
    def forward(self,  q_proj_weight: torch.Tensor,k_proj_weight: torch.Tensor,v_proj_weight: torch.Tensor,o_proj_weight: torch.Tensor,in_features: torch.Tensor,max_sequence_len):
        *batch_shape, seq_len, d_in = in_features.shape
        q=F.linear(in_features, q_proj_weight)  # [..., seq_len, d_k]
        k=F.linear(in_features, k_proj_weight) 
        v=F.linear(in_features, v_proj_weight) 

        q=q.view(*batch_shape,seq_len,self.num_heads,self.dk).transpose(-2,-3)
        k=k.view(*batch_shape,seq_len,self.num_heads,self.dk).transpose(-2,-3)
        v=v.view(*batch_shape,seq_len,self.num_heads,self.dk).transpose(-2,-3)

        mask=torch.tril(torch.ones(max_sequence_len,max_sequence_len),diagonal=0).bool()
        causal_mask=mask.squeeze(0).squeeze(0)
        out=scaled_dot_product(q,k,v,causal_mask)
        out = out.transpose(1, 2).reshape(*batch_shape, seq_len, self.d_model) 
        out=F.linear(out,o_proj_weight)
        return out 

class CausalMultiheadsSelfAttentionWithRope(nn.Module):
    """
    Args:
        d_model (int): Dimensionality of the feedforward input and output.
        num_heads (int): Number of heads to use in multi-headed attention.
        max_seq_len (int): Maximum sequence length to pre-cache if your implementation does that.
        theta (float): RoPE parameter.
        q_proj_weight (Float[Tensor, "d_k d_in"]): Weights for the Q projection
        k_proj_weight (Float[Tensor, "d_k d_in"]): Weights for the K projection
        v_proj_weight (Float[Tensor, "d_k d_in"]): Weights for the V projection
        o_proj_weight (Float[Tensor, "d_model d_v"]): Weights for the output projection
        in_features (Float[Tensor, "... sequence_length d_in"]): Tensor to run your implementation on.
        token_positions (Int[Tensor, " ... sequence_length"] | None): Optional tensor with the positions of the tokens
    """
    def __init__(self, d_model: int, num_heads: int,theta: int ,max_sequence_len:int, device=None, dtype=None):
        super().__init__()
        self.d_model=d_model
        self.num_heads=num_heads
        self.theta=theta
        self.max_seq_len=max_sequence_len

        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.dk = self.dv = d_model//num_heads

    
    def forward(self,  q_proj_weight: torch.Tensor,k_proj_weight: torch.Tensor,v_proj_weight: torch.Tensor,o_proj_weight: torch.Tensor,in_features: torch.Tensor, token_positions):
        rope=ROPE(self.theta,self.dk,self.max_seq_len)

        *batch_shape, seq_len, d_in = in_features.shape
        q=F.linear(in_features, q_proj_weight)  # [..., seq_len, d_k]
        k=F.linear(in_features, k_proj_weight) 
        v=F.linear(in_features, v_proj_weight) 

        q=q.view(*batch_shape,seq_len,self.num_heads,self.dk).transpose(-2,-3)# [...,num_heads, seq_len, d_k]
        k=k.view(*batch_shape,seq_len,self.num_heads,self.dk).transpose(-2,-3)
        v=v.view(*batch_shape,seq_len,self.num_heads,self.dk).transpose(-2,-3)

        q=rope(q,token_positions)
        k=rope(k,token_positions)

        mask=torch.tril(torch.ones(seq_len,seq_len),diagonal=0).bool()
        causal_mask=mask.unsqueeze(0).unsqueeze(0)
        out=scaled_dot_product(q,k,v,causal_mask)
        out = out.transpose(1, 2).reshape(*batch_shape, seq_len, self.d_model) 
        out=F.linear(out,o_proj_weight)
        return out 

class TransformerBlock(nn.Module):
    def __init__(self, d_model: int,num_heads: int,d_ff: int,max_seq_len: int,theta: float):
        super().__init__()
        self.d_model=d_model
        self.num_heads=num_heads
        self.d_ff=d_ff
        self.max_seq_len=max_seq_len
        self.theta=theta

        self.norm1=RMSNorm(d_model)
        self.attn=CausalMultiheadsSelfAttentionWithRope(d_model,num_heads,theta,max_seq_len)
        self.norm2=RMSNorm(d_model)
        self.ffn=SWIGLU(d_model,d_ff)

    def forward(self,weights, in_features):
        *batch_shape, seq_len, d_in = in_features.shape
        
        ## 多头注意力
        with torch.no_grad():
            self.norm1.scale.copy_(weights["ln1.weight"])
        res_norm1=self.norm1(in_features)

        token_positions = torch.arange(seq_len, device=in_features.device)  # [seq_len]
        token_positions = token_positions.unsqueeze(0).expand(*batch_shape, seq_len)  # [batch_size, seq_len]
        
        res_attn=self.attn(weights["attn.q_proj.weight"],weights["attn.k_proj.weight"],weights["attn.v_proj.weight"],weights["attn.output_proj.weight"],res_norm1,token_positions)

        res_b1=in_features+res_attn
        
        ## FFN
        with torch.no_grad():
            self.norm2.scale.copy_(weights["ln2.weight"])
        res_norm2=self.norm2(res_b1)

        with torch.no_grad():
            self.ffn.w1.copy_(weights["ffn.w1.weight"])
            self.ffn.w2.copy_(weights["ffn.w2.weight"])
            self.ffn.w3.copy_(weights["ffn.w3.weight"])

        res_ffn=self.ffn(res_norm2)
        res=res_b1+res_ffn
        return res

class Transformer_Lm(nn.Module):
    def __init__(self, 
                 vocab_size: int,
                 context_length: int,
                 d_model: int,
                 num_layers: int,
                 num_heads: int,
                 d_ff: int,
                 rope_theta: float,):
        super().__init__()
        self.vocab_size=vocab_size
        self.context_length=context_length
        self.d_model=d_model
        self.num_layers=num_layers
        self.num_heads=num_heads
        self.d_ff=d_ff
        self.rope_theta=rope_theta

        
        self.embedding=Embedding(vocab_size,d_model)
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff, context_length, rope_theta)
            for _ in range(num_layers)
        ])
        self.norm=RMSNorm(d_model)
        self.linear=Linear(d_model,vocab_size)
    
    def forward(self, weights: dict[str, Tensor], in_indices: Int[Tensor, " batch_size sequence_length"]):
        with torch.no_grad():
            self.embedding.W.copy_(weights["token_embeddings.weight"])
        emb=self.embedding(in_indices)
        res_block=emb
        for i,block in enumerate(self.blocks):
            layer_weights={
                key.replace(f"layers.{i}.",""):val
                for key, val in weights.items()
                if key.startswith(f"layers.{i}.")
            }
            res_block=block(layer_weights,res_block)

        with torch.no_grad():
            self.norm.scale.copy_(weights["ln_final.weight"])
        res_norm=self.norm(res_block)

        with torch.no_grad():
            self.linear.W.copy_(weights["lm_head.weight"])
        res_linear=self.linear(res_norm)

        res_lm=soft_max(res_linear,-1)

        return res_linear



            
        
        
        
            
        
        
